{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from numbers import Number\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import visdom\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lib.dist as dist\n",
    "import lib.utils as utils\n",
    "import lib.datasets as dset\n",
    "from lib.flows import FactorialNormalizingFlow\n",
    "\n",
    "from elbo_decomposition import elbo_decomposition\n",
    "# from plot_latent_vs_true import plot_vs_gt_shapes, plot_vs_gt_faces  # noqa: F401\n",
    "\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, output_dim)\n",
    "\n",
    "        self.conv_z = nn.Conv2d(64, output_dim, 4, 1, 0)\n",
    "\n",
    "        # setup the non-linearity\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.view(-1, 64 * 64)\n",
    "        h = self.act(self.fc1(h))\n",
    "        h = self.act(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "        z = h.view(x.size(0), self.output_dim)\n",
    "        return z\n",
    "\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1200, 1200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1200, 1200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1200, 4096)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = z.view(z.size(0), -1)\n",
    "        h = self.net(h)\n",
    "        mu_img = h.view(z.size(0), 1, 64, 64)\n",
    "        return mu_img\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 4, 2, 1)  # 32 x 32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 4, 2, 1)  # 16 x 16\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 4, 2, 1)  # 8 x 8\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 4, 2, 1)  # 4 x 4\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 512, 4)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.conv_z = nn.Conv2d(512, output_dim, 1)\n",
    "\n",
    "        # setup the non-linearity\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.view(-1, 1, 64, 64)\n",
    "        h = self.act(self.bn1(self.conv1(h)))\n",
    "        h = self.act(self.bn2(self.conv2(h)))\n",
    "        h = self.act(self.bn3(self.conv3(h)))\n",
    "        h = self.act(self.bn4(self.conv4(h)))\n",
    "        h = self.act(self.bn5(self.conv5(h)))\n",
    "        z = self.conv_z(h).view(x.size(0), self.output_dim)\n",
    "        return z\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(input_dim, 512, 1, 1, 0)  # 1 x 1\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv2 = nn.ConvTranspose2d(512, 64, 4, 1, 0)  # 4 x 4\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)  # 8 x 8\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 32, 4, 2, 1)  # 16 x 16\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.ConvTranspose2d(32, 32, 4, 2, 1)  # 32 x 32\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.conv_final = nn.ConvTranspose2d(32, 1, 4, 2, 1)\n",
    "\n",
    "        # setup the non-linearity\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        h = self.act(self.bn1(self.conv1(h)))\n",
    "        h = self.act(self.bn2(self.conv2(h)))\n",
    "        h = self.act(self.bn3(self.conv3(h)))\n",
    "        h = self.act(self.bn4(self.conv4(h)))\n",
    "        h = self.act(self.bn5(self.conv5(h)))\n",
    "        mu_img = self.conv_final(h)\n",
    "        return mu_img\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, use_cuda=False, prior_dist=dist.Normal(), q_dist=dist.Normal(),\n",
    "                 include_mutinfo=True, tcvae=False, conv=False, mss=False):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.include_mutinfo = include_mutinfo\n",
    "        self.tcvae = tcvae\n",
    "        self.lamb = 0\n",
    "        self.beta = 1\n",
    "        self.mss = mss\n",
    "        self.x_dist = dist.Bernoulli()\n",
    "\n",
    "        # Model-specific\n",
    "        # distribution family of p(z)\n",
    "        self.prior_dist = prior_dist\n",
    "        self.q_dist = q_dist\n",
    "        # hyperparameters for prior p(z)\n",
    "        self.register_buffer('prior_params', torch.zeros(self.z_dim, 2))\n",
    "\n",
    "        # create the encoder and decoder networks\n",
    "        if conv:\n",
    "            self.encoder = ConvEncoder(z_dim * self.q_dist.nparams)\n",
    "            self.decoder = ConvDecoder(z_dim)\n",
    "        else:\n",
    "            self.encoder = MLPEncoder(z_dim * self.q_dist.nparams)\n",
    "            self.decoder = MLPDecoder(z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "\n",
    "    # return prior parameters wrapped in a suitable Variable\n",
    "    def _get_prior_params(self, batch_size=1):\n",
    "        expanded_size = (batch_size,) + self.prior_params.size()\n",
    "        prior_params = Variable(self.prior_params.expand(expanded_size))\n",
    "        return prior_params\n",
    "\n",
    "    # samples from the model p(x|z)p(z)\n",
    "    def model_sample(self, batch_size=1):\n",
    "        # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "        prior_params = self._get_prior_params(batch_size)\n",
    "        zs = self.prior_dist.sample(params=prior_params)\n",
    "        # decode the latent code z\n",
    "        x_params = self.decoder.forward(zs)\n",
    "        return x_params\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def encode(self, x):\n",
    "        x = x.view(x.size(0), 1, 64, 64)\n",
    "        # use the encoder to get the parameters used to define q(z|x)\n",
    "        z_params = self.encoder.forward(x).view(x.size(0), self.z_dim, self.q_dist.nparams)\n",
    "        # sample the latent code z\n",
    "        zs = self.q_dist.sample(params=z_params)\n",
    "        return zs, z_params\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_params = self.decoder.forward(z).view(z.size(0), 1, 64, 64)\n",
    "        xs = self.x_dist.sample(params=x_params)\n",
    "        return xs, x_params\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        zs, z_params = self.encode(x)\n",
    "        xs, x_params = self.decode(zs)\n",
    "        return xs, x_params, zs, z_params\n",
    "\n",
    "    def _log_importance_weight_matrix(self, batch_size, dataset_size):\n",
    "        N = dataset_size\n",
    "        M = batch_size - 1\n",
    "        strat_weight = (N - M) / (N * M)\n",
    "        W = torch.Tensor(batch_size, batch_size).fill_(1 / M)\n",
    "        W.view(-1)[::M+1] = 1 / N\n",
    "        W.view(-1)[1::M+1] = strat_weight\n",
    "        W[M-1, 0] = strat_weight\n",
    "        return W.log()\n",
    "\n",
    "    def elbo(self, x, dataset_size):\n",
    "        # log p(x|z) + log p(z) - log q(z|x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 1, 64, 64)\n",
    "        prior_params = self._get_prior_params(batch_size)\n",
    "        x_recon, x_params, zs, z_params = self.reconstruct_img(x)\n",
    "        logpx = self.x_dist.log_density(x, params=x_params).view(batch_size, -1).sum(1)\n",
    "        logpz = self.prior_dist.log_density(zs, params=prior_params).view(batch_size, -1).sum(1)\n",
    "        logqz_condx = self.q_dist.log_density(zs, params=z_params).view(batch_size, -1).sum(1)\n",
    "\n",
    "        elbo = logpx + logpz - logqz_condx\n",
    "\n",
    "        if self.beta == 1 and self.include_mutinfo and self.lamb == 0:\n",
    "            return elbo, elbo.detach()\n",
    "\n",
    "        # compute log q(z) ~= log 1/(NM) sum_m=1^M q(z|x_m) = - log(MN) + logsumexp_m(q(z|x_m))\n",
    "        _logqz = self.q_dist.log_density(\n",
    "            zs.view(batch_size, 1, self.z_dim),\n",
    "            z_params.view(1, batch_size, self.z_dim, self.q_dist.nparams)\n",
    "        )\n",
    "\n",
    "        if not self.mss:\n",
    "            # minibatch importance sampling\n",
    "            logqz_prodmarginals = (logsumexp(_logqz, dim=1, keepdim=False) - math.log(batch_size * dataset_size)).sum(1)\n",
    "            logqz = (logsumexp(_logqz.sum(2), dim=1, keepdim=False) - math.log(batch_size * dataset_size))\n",
    "        else:\n",
    "            # minibatch stratified sampling\n",
    "            logiw_matrix = Variable(self._log_importance_weight_matrix(batch_size, dataset_size).type_as(_logqz.data))\n",
    "            logqz = logsumexp(logiw_matrix + _logqz.sum(2), dim=1, keepdim=False)\n",
    "            logqz_prodmarginals = logsumexp(\n",
    "                logiw_matrix.view(batch_size, batch_size, 1) + _logqz, dim=1, keepdim=False).sum(1)\n",
    "\n",
    "        if not self.tcvae:\n",
    "            if self.include_mutinfo:\n",
    "                modified_elbo = logpx - self.beta * (\n",
    "                    (logqz_condx - logpz) -\n",
    "                    self.lamb * (logqz_prodmarginals - logpz)\n",
    "                )\n",
    "            else:\n",
    "                modified_elbo = logpx - self.beta * (\n",
    "                    (logqz - logqz_prodmarginals) +\n",
    "                    (1 - self.lamb) * (logqz_prodmarginals - logpz)\n",
    "                )\n",
    "        else:\n",
    "            if self.include_mutinfo:\n",
    "                modified_elbo = logpx - \\\n",
    "                    (logqz_condx - logqz) - \\\n",
    "                    self.beta * (logqz - logqz_prodmarginals) - \\\n",
    "                    (1 - self.lamb) * (logqz_prodmarginals - logpz)\n",
    "            else:\n",
    "                modified_elbo = logpx - \\\n",
    "                    self.beta * (logqz - logqz_prodmarginals) - \\\n",
    "                    (1 - self.lamb) * (logqz_prodmarginals - logpz)\n",
    "\n",
    "        return modified_elbo, elbo.detach()\n",
    "\n",
    "\n",
    "def logsumexp(value, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable implementation of the operation\n",
    "\n",
    "    value.exp().sum(dim, keepdim).log()\n",
    "    \"\"\"\n",
    "    if dim is not None:\n",
    "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "        value0 = value - m\n",
    "        if keepdim is False:\n",
    "            m = m.squeeze(dim)\n",
    "        return m + torch.log(torch.sum(torch.exp(value0),\n",
    "                                       dim=dim, keepdim=keepdim))\n",
    "    else:\n",
    "        m = torch.max(value)\n",
    "        sum_exp = torch.sum(torch.exp(value - m))\n",
    "        if isinstance(sum_exp, Number):\n",
    "            return m + math.log(sum_exp)\n",
    "        else:\n",
    "            return m + torch.log(sum_exp)\n",
    "\n",
    "\n",
    "# for loading and batching datasets\n",
    "def setup_data_loaders(args, use_cuda=False):\n",
    "    if args.dataset == 'shapes':\n",
    "        train_set = dset.Shapes()\n",
    "    elif args.dataset == 'faces':\n",
    "        train_set = dset.Faces()\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset ' + str(args.dataset))\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': use_cuda}\n",
    "    train_loader = DataLoader(dataset=train_set,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "win_samples = None\n",
    "win_test_reco = None\n",
    "win_latent_walk = None\n",
    "win_train_elbo = None\n",
    "\n",
    "\n",
    "def display_samples(model, x, vis):\n",
    "    global win_samples, win_test_reco, win_latent_walk\n",
    "\n",
    "    # plot random samples\n",
    "    sample_mu = model.model_sample(batch_size=100).sigmoid()\n",
    "    sample_mu = sample_mu\n",
    "    images = list(sample_mu.view(-1, 1, 64, 64).data.cpu())\n",
    "    win_samples = vis.images(images, 10, 2, opts={'caption': 'samples'}, win=win_samples)\n",
    "\n",
    "    # plot the reconstructed distribution for the first 50 test images\n",
    "    test_imgs = x[:50, :]\n",
    "    _, reco_imgs, zs, _ = model.reconstruct_img(test_imgs)\n",
    "    reco_imgs = reco_imgs.sigmoid()\n",
    "    test_reco_imgs = torch.cat([\n",
    "        test_imgs.view(1, -1, 64, 64), reco_imgs.view(1, -1, 64, 64)], 0).transpose(0, 1)\n",
    "    win_test_reco = vis.images(\n",
    "        list(test_reco_imgs.contiguous().view(-1, 1, 64, 64).data.cpu()), 10, 2,\n",
    "        opts={'caption': 'test reconstruction image'}, win=win_test_reco)\n",
    "\n",
    "    # plot latent walks (change one variable while all others stay the same)\n",
    "    zs = zs[0:3]\n",
    "    batch_size, z_dim = zs.size()\n",
    "    xs = []\n",
    "    delta = torch.autograd.Variable(torch.linspace(-2, 2, 7), volatile=True).type_as(zs)\n",
    "    for i in range(z_dim):\n",
    "        vec = Variable(torch.zeros(z_dim)).view(1, z_dim).expand(7, z_dim).contiguous().type_as(zs)\n",
    "        vec[:, i] = 1\n",
    "        vec = vec * delta[:, None]\n",
    "        zs_delta = zs.clone().view(batch_size, 1, z_dim)\n",
    "        zs_delta[:, :, i] = 0\n",
    "        zs_walk = zs_delta + vec[None]\n",
    "        xs_walk = model.decoder.forward(zs_walk.view(-1, z_dim)).sigmoid()\n",
    "        xs.append(xs_walk)\n",
    "\n",
    "    xs = list(torch.cat(xs, 0).data.cpu())\n",
    "    win_latent_walk = vis.images(xs, 7, 2, opts={'caption': 'latent walk'}, win=win_latent_walk)\n",
    "\n",
    "\n",
    "def plot_elbo(train_elbo, vis):\n",
    "    global win_train_elbo\n",
    "    win_train_elbo = vis.line(torch.Tensor(train_elbo), opts={'markers': True}, win=win_train_elbo)\n",
    "\n",
    "\n",
    "def anneal_kl(args, vae, iteration):\n",
    "    if args.dataset == 'shapes':\n",
    "        warmup_iter = 7000\n",
    "    elif args.dataset == 'faces':\n",
    "        warmup_iter = 2500\n",
    "\n",
    "    if args.lambda_anneal:\n",
    "        vae.lamb = max(0, 0.95 - 1 / warmup_iter * iteration)  # 1 --> 0\n",
    "    else:\n",
    "        vae.lamb = 0\n",
    "    if args.beta_anneal:\n",
    "        vae.beta = min(args.beta, args.beta / warmup_iter * iteration)  # 0 --> 1\n",
    "    else:\n",
    "        vae.beta = args.beta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [-d {shapes,faces}] [-dist {normal,laplace,flow}]\n",
      "                   [-n NUM_EPOCHS] [-b BATCH_SIZE] [-l LEARNING_RATE]\n",
      "                   [-z LATENT_DIM] [--beta BETA] [--tcvae] [--exclude-mutinfo]\n",
      "                   [--beta-anneal] [--lambda-anneal] [--mss] [--conv]\n",
      "                   [--gpu GPU] [--visdom] [--save SAVE] [--log_freq LOG_FREQ]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1001/jupyter/kernel-ea889529-756c-448a-b04f-c8353d129e41.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weizhen/anaconda3/envs/pytorch/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class arguments:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'shapes'\n",
    "        self.dist = 'normal'\n",
    "        self.num_epochs = 50\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 1e-3\n",
    "        self.latent_dim = 10\n",
    "        self.beta =1.\n",
    "        self.tcvae = True\n",
    "        self.exclude_mutinfo = True\n",
    "        self.beta_anneal = True\n",
    "        self.lambda_anneal = True\n",
    "        self.mss = True\n",
    "        self.conv = True\n",
    "\n",
    "args = arguments()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 11GB. Buy new RAM! at /opt/conda/conda-bld/pytorch-cpu_1518282373170/work/torch/lib/TH/THGeneral.c:253",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e1dddc3ce375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# setup the VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-560f5ed25c52>\u001b[0m in \u001b[0;36msetup_data_loaders\u001b[0;34m(args, use_cuda)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetup_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'shapes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mShapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'faces'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/beta-tcvae-public/lib/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_zip)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_zip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imgs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mfloat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34mr\"\"\"Casts this tensor to float type\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 11GB. Buy new RAM! at /opt/conda/conda-bld/pytorch-cpu_1518282373170/work/torch/lib/TH/THGeneral.c:253"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "#     # parse command line arguments\n",
    "#     parser = argparse.ArgumentParser(description=\"parse args\")\n",
    "#     parser.add_argument('-d', '--dataset', default='shapes', type=str, help='dataset name',\n",
    "#         choices=['shapes', 'faces'])\n",
    "#     parser.add_argument('-dist', default='normal', type=str, choices=['normal', 'laplace', 'flow'])\n",
    "#     parser.add_argument('-n', '--num-epochs', default=50, type=int, help='number of training epochs')\n",
    "#     parser.add_argument('-b', '--batch-size', default=2048, type=int, help='batch size')\n",
    "#     parser.add_argument('-l', '--learning-rate', default=1e-3, type=float, help='learning rate')\n",
    "#     parser.add_argument('-z', '--latent-dim', default=10, type=int, help='size of latent dimension')\n",
    "#     parser.add_argument('--beta', default=1, type=float, help='ELBO penalty term')\n",
    "#     parser.add_argument('--tcvae', action='store_true')\n",
    "#     parser.add_argument('--exclude-mutinfo', action='store_true')\n",
    "#     parser.add_argument('--beta-anneal', action='store_true')\n",
    "#     parser.add_argument('--lambda-anneal', action='store_true')\n",
    "#     parser.add_argument('--mss', action='store_true', help='use the improved minibatch estimator')\n",
    "#     parser.add_argument('--conv', action='store_true')\n",
    "#     parser.add_argument('--gpu', type=int, default=0)\n",
    "#     parser.add_argument('--visdom', action='store_true', help='whether plotting in visdom is desired')\n",
    "#     parser.add_argument('--save', default='test1')\n",
    "#     parser.add_argument('--log_freq', default=200, type=int, help='num iterations per log')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     torch.cuda.set_device(args.gpu)\n",
    "\n",
    "# data loader\n",
    "train_loader = setup_data_loaders(args, use_cuda=True)\n",
    "\n",
    "# setup the VAE\n",
    "if args.dist == 'normal':\n",
    "    prior_dist = dist.Normal()\n",
    "    q_dist = dist.Normal()\n",
    "elif args.dist == 'laplace':\n",
    "    prior_dist = dist.Laplace()\n",
    "    q_dist = dist.Laplace()\n",
    "elif args.dist == 'flow':\n",
    "    prior_dist = FactorialNormalizingFlow(dim=args.latent_dim, nsteps=32)\n",
    "    q_dist = dist.Normal()\n",
    "\n",
    "vae = VAE(z_dim=args.latent_dim, use_cuda=True, prior_dist=prior_dist, q_dist=q_dist,\n",
    "    include_mutinfo=not args.exclude_mutinfo, tcvae=args.tcvae, conv=args.conv, mss=args.mss)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom(env=args.save, port=4500)\n",
    "\n",
    "train_elbo = []\n",
    "\n",
    "# training loop\n",
    "dataset_size = len(train_loader.dataset)\n",
    "num_iterations = len(train_loader) * args.num_epochs\n",
    "iteration = 0\n",
    "# initialize loss accumulator\n",
    "elbo_running_mean = utils.RunningAverageMeter()\n",
    "while iteration < num_iterations:\n",
    "    for i, x in enumerate(train_loader):\n",
    "        iteration += 1\n",
    "        batch_time = time.time()\n",
    "        vae.train()\n",
    "        anneal_kl(args, vae, iteration)\n",
    "        optimizer.zero_grad()\n",
    "        # transfer to GPU\n",
    "        x = x.cuda(async=True)\n",
    "        # wrap the mini-batch in a PyTorch Variable\n",
    "        x = Variable(x)\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        obj, elbo = vae.elbo(x, dataset_size)\n",
    "        if utils.isnan(obj).any():\n",
    "            raise ValueError('NaN spotted in objective.')\n",
    "        obj.mean().mul(-1).backward()\n",
    "        elbo_running_mean.update(elbo.mean().data[0])\n",
    "        optimizer.step()\n",
    "\n",
    "        # report training diagnostics\n",
    "        if iteration % args.log_freq == 0:\n",
    "            train_elbo.append(elbo_running_mean.avg)\n",
    "            print('[iteration %03d] time: %.2f \\tbeta %.2f \\tlambda %.2f training ELBO: %.4f (%.4f)' % (\n",
    "                iteration, time.time() - batch_time, vae.beta, vae.lamb,\n",
    "                elbo_running_mean.val, elbo_running_mean.avg))\n",
    "\n",
    "            vae.eval()\n",
    "\n",
    "            # plot training and test ELBOs\n",
    "            if args.visdom:\n",
    "                display_samples(vae, x, vis)\n",
    "                plot_elbo(train_elbo, vis)\n",
    "\n",
    "            utils.save_checkpoint({\n",
    "                'state_dict': vae.state_dict(),\n",
    "                'args': args}, args.save, 0)\n",
    "            eval('plot_vs_gt_' + args.dataset)(vae, train_loader.dataset,\n",
    "                os.path.join(args.save, 'gt_vs_latent_{:05d}.png'.format(iteration)))\n",
    "\n",
    "# Report statistics after training\n",
    "vae.eval()\n",
    "utils.save_checkpoint({\n",
    "    'state_dict': vae.state_dict(),\n",
    "    'args': args}, args.save, 0)\n",
    "dataset_loader = DataLoader(train_loader.dataset, batch_size=1000, num_workers=1, shuffle=False)\n",
    "logpx, dependence, information, dimwise_kl, analytical_cond_kl, marginal_entropies, joint_entropy = \\\n",
    "    elbo_decomposition(vae, dataset_loader)\n",
    "torch.save({\n",
    "    'logpx': logpx,\n",
    "    'dependence': dependence,\n",
    "    'information': information,\n",
    "    'dimwise_kl': dimwise_kl,\n",
    "    'analytical_cond_kl': analytical_cond_kl,\n",
    "    'marginal_entropies': marginal_entropies,\n",
    "    'joint_entropy': joint_entropy\n",
    "}, os.path.join(args.save, 'elbo_decomposition.pth'))\n",
    "eval('plot_vs_gt_' + args.dataset)(vae, dataset_loader.dataset, os.path.join(args.save, 'gt_vs_latent.png'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal (0.000, 1.000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
